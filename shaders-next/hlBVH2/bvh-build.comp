#version 460 core
#extension GL_GOOGLE_include_directive : enable

#ifdef NVIDIA_PLATFORM
#define WORK_SIZE_BND 1024
#else 
#define WORK_SIZE_BND 1024
#endif

#define BVH_BUILD
#define BVH_CREATION

#include "../include/driver.glsl"
#include "../include/structs.glsl"
#include "../include/mathlib.glsl"
#include "../include/ballotlib.glsl"
#include "../include/vertex.glsl"
#include "./submodules/includes.glsl"

// shared memory counters
shared int _counters[8];
#define cBuffer _counters[3]

// define function for increment
initAtomicSubgroupIncFunctionTarget(_counters[WHERE], _aCounterInc, 1, int)
initAtomicSubgroupIncFunctionTarget(vtCounters[WHERE], vtCountersInc, 1, int)
int lCounterInc() {return vtCountersInc(0)*2;};
int cCounterInc() {return vtCountersInc(1);};

int aCounterInc(){return _aCounterInc(cBuffer*4);}
uint wID(in uint t){return t*gl_NumWorkGroups.x+gl_WorkGroupID.x;}

#define asize_ _counters[(1-cBuffer)*4]
#define asize_inv_ _counters[cBuffer*4]
#define asize _counters[7]

#ifdef FIRST_STEP
#define osize asize
#define wsize asize
#else
#define osize _counters[6]
#define wsize _counters[5]
#endif

#include "./submodules/bvh-build-general.glsl" // unified functions
layout ( local_size_x = WORK_SIZE_BND ) in;

// I have no idea for optimize registers
void main() {
#define threadID Local_Idx
#define groupSize gl_WorkGroupSize.x

    // lane-based
    //const uint gS = groupSize >> 1, iT = threadID >> 1;
    //uint gS = 0u, iT = 0u, uID = 0u, uWD = 0u;
    uint gS = 0u, iT = 0u, uWD = 0u;
    gS = groupSize >> 1, iT = threadID >> 1;
    int sD = int(threadID & 1);

    LGROUP_BARRIER
    if (threadID < 8) { _counters[threadID] = 0; }
    LGROUP_BARRIER

    // create initial (root) node
    if (threadID == 0) {
#ifdef FIRST_STEP
        //int hid = lCounterInc();
        int hid = 0;
        imageStore(bvhMeta, hid+0, ivec4(1, bvhBlock.leafCount, 0, 0));
        imageStore(bvhMeta, hid+1, ivec4(0, 0, 0, 0));
        //bvhMeta[hid+0] = ivec4(1, bvhBlock.leafCount, 0, 0);
        //bvhMeta[hid+1] = ivec4(0, 0, 0, 0);
        Actives[aCounterInc()][cBuffer] = hid+1;
        //asize = asize_inv_;
#else
        //asize_inv_ = aCounter[  cBuffer]/int(gl_NumWorkGroups.x>>1), asize_ = aCounter[1-cBuffer]/int(gl_NumWorkGroups.x>>1);
        asize_inv_ = aCounter[  cBuffer], asize_ = aCounter[1-cBuffer];
#endif
    }
    
    // building BVH
    [[dependency_infinite]]
    for (int m=0;m<65536;m++) {
        LGROUP_BARRIER

#ifdef FIRST_STEP // limit by few elements
        if (m >= 4) { break; }     
#endif

        // swap buffers emulation
        if (threadID == 0) { 
                cBuffer = 1-cBuffer; asize_inv_ = 0; asize = atomicExchange(asize_, 0);
#ifdef FIRST_STEP
//                wsize = asize, osize = wsize;
#else
                wsize = asize * (m == 0 ? 1 : int(gl_NumWorkGroups.x)), osize = wsize / int(gl_NumWorkGroups.x); 
#endif
        }
        LGROUP_BARRIER

        //IFALL (osize >= (gS*largeStageThreshold) || osize <= 0) { break; }
        IFALL (osize <= 0) { break; } // remove count limiter

        // split nodes
        for (uint fT=0;fT<osize;fT+=gS) {
            // subgroup barrier
            SB_BARRIER

            // index of node element
            //const uint uID = fT + iT;
            uWD = wID(fT + iT);

            [[flatten]]
            IFALL (uWD >= wsize) break; // split prefixed elements

            // get spared prefix
            int fID = Actives[uWD][1-cBuffer]-1;
            if (sD == 0) Actives[uWD][1-cBuffer] = 0;

            // split sibling nodes
            [[flatten]]
            if (uWD < wsize && fID >= 0) { splitNode(fID, sD); }
        }
    }

    LGROUP_BARRIER

#ifdef FIRST_STEP
    if (threadID == 0) { aCounter[cBuffer] = asize_inv_, aCounter[1-cBuffer] = asize_; }
#endif
}
