#version 460 core
#extension GL_GOOGLE_include_directive : enable

#include "./includes.glsl"

layout (local_size_x = BLOCK_SIZE) in;

shared uint localHistogram[RADICES];

// wide of 128 elements, each element should be in uint16_t, but for compatibility used uint32_t
#ifdef PREFER_UNPACKED 
shared lowp uvec4 _data[Wave_Size];
#else
shared uint_rdc_wave_lcm _data[Wave_Size];
#endif

shared blocks_info blocks;
#define key _data[Lane_Idx]

initSubgroupIncFunctionTargetQuad(localHistogram[WHERE], countHistogram, 1, uint, uvec4)

#define bcount blocks.count

void main() {
    const lowp uint Radice_Idx = gl_WorkGroupID.y * Wave_Count_RX + Wave_Idx;
    const uint w = Wave_Idx; // should be reference, not register

    // clear histogram of block (planned distribute threads)
    [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=WRK_SIZE_RT) {
        const lowp uint radice = uint(rk + Radice_Idx);
        [[flatten]] if (radice < RADICES) localHistogram[radice] = 0;
    };
    [[flatten]] if (Local_Idx == 0) blocks = get_blocks_info(push_block.NumKeys), bcount = min(blocks.count, 524288u);
    LGROUP_BARRIER
    IFANY (bcount <= 0) return;

    // use SIMD lanes for calculate histograms
    WPTR4 addr = WPTR4(0,1,2,3)*Wave_Size_RT.xxxx + WPTR4(blocks.offset.xxxx) + WPTR4(Lane_Idx.xxxx);
    [[dependency_infinite]] for ( uint wk = 0; wk < bcount; wk++ ) {
        LGROUP_BARRIER

        const bvec4 validAddress = lessThan(addr, blocks.limit.xxxx); IFALL(all(not(validAddress))) break;
        [[flatten]] if (w < 4u) { key[w] = BFE(validAddress[w] ? KeyIn[addr[w]] : OutOfRange, (push_block.Shift)*BITS_PER_PASS, BITS_PER_PASS); };
        LGROUP_BARRIER

        // WARP-optimized histogram calculation
        [[unroll]] for (lowp uint rk=0u;rk<RADICES;rk+=WRK_SIZE_RT) {
            const BOTTLE_TYPE radice = BOTTLE_TYPE(rk + Radice_Idx);
            const bvec4 owned = and(equal(BOTTLE_TYPE_VEC(upfunc(key)), radice.xxxx), validAddress);
             [[flatten]] if (any(owned)) countHistogram(uint(radice), owned);
             IFALL (all(or((radice >= RADICES).xxxx, not(validAddress)))) break;
        };
        addr += ( Wave_Size_RT << 2u );
    };

    // resolve histograms (planned distribute threads) 
    LGROUP_BARRIER

    [[unroll]] for (uint rk=0u;rk<RADICES;rk+=WRK_SIZE_RT) {
         const lowp uint radice = uint(rk + Radice_Idx);
         [[flatten]] if (radice < RADICES) {
            PrefixSum[radice + RADICES * gl_WorkGroupID.x] = localHistogram[radice];
            Histogram[radice + RADICES * gl_WorkGroupID.x] = localHistogram[radice];
        };
    };
};
